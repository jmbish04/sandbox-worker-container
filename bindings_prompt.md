Here is the prompt for the next stage of your worker, incorporating the robust D1 backend, agentic orchestration, and advanced storage architecture you described.

-----

### Task: Evolve `repo-agent-worker` into a Resilient, Observable, and Agentic Platform

Your objective is to refactor the `repo-agent-worker` into a stateful, resilient, and fully observable system. This involves implementing a D1 database for comprehensive logging, building an agentic orchestration layer using Cloudflare Agents and Durable Objects, and leveraging KV/R2 for caching and asset management.

This prompt builds directly on the foundation from the previous prompt (Hono, Sandbox SDK, WebSocket, and static frontend).

-----

### 1\. Phase 1: D1 Database for Traceability

First, define the D1 database schema to provide full traceability for every request and operation.

**A. D1 Migration (`migrations/0001_initial_schema.sql`)**

Create the following SQL migration file:

```sql
-- 1. Repeatable tasks (for the "user journeys" / feature paths)
CREATE TABLE IF NOT EXISTS repeatable_tasks (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    pathway TEXT NOT NULL UNIQUE, -- 'error_recreation', 'solution_validation', 'testing', etc.
    name TEXT NOT NULL,
    description TEXT,
    prompt_template TEXT, -- A template for the user to fill out
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Seed the table with your defined pathways
INSERT INTO repeatable_tasks (pathway, name, description, prompt_template) VALUES
('error_recreation', 'Error Reproducer', 'Clones a repo and attempts to reproduce a bug.', '{"repo_url": "...", "bug_description": "..."}'),
('solution_validation', 'Solution Validator', 'Applies a patch and runs tests to validate a fix.', '{"repo_url": "...", "branch": "...", "patch": "..."}'),
('testing', 'E2E/Unit Testing', 'Runs a test suite against a repo.', '{"repo_url": "...", "test_mode": "both", "frontend_prompt": "...", "backend_prompt": "..."}');

-- 2. Main request log, serves as the parent for all traceability
CREATE TABLE IF NOT EXISTS task_requests (
    id TEXT PRIMARY KEY, -- A CUID or UUID generated by the worker
    pathway TEXT NOT NULL,
    status TEXT NOT NULL DEFAULT 'pending', -- 'pending', 'running', 'success', 'failed'
    initial_prompt TEXT NOT NULL,
    repo_url TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    ended_at TIMESTAMP
);
CREATE INDEX IF NOT EXISTS idx_task_requests_status ON task_requests (status);
CREATE INDEX IF NOT EXISTS idx_task_requests_pathway ON task_requests (pathway);

-- 3. AI operation logs, linked to a task request
CREATE TABLE IF NOT EXISTS ai_operation_logs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    task_request_id TEXT NOT NULL,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    agent_name TEXT NOT NULL, -- e.g., 'ValidationAgent', 'TestingAgent'
    ai_provider TEXT, -- 'workers-ai', 'gemini', etc.
    thought TEXT, -- The "agentic thinking"
    prompt TEXT,
    response TEXT,
    FOREIGN KEY (task_request_id) REFERENCES task_requests(id) ON DELETE CASCADE
);
CREATE INDEX IF NOT EXISTS idx_ai_operation_logs_task_id ON ai_operation_logs (task_request_id);

-- 4. Container operation logs, linked to a task request
CREATE TABLE IF NOT EXISTS container_operation_logs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    task_request_id TEXT NOT NULL,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    stream TEXT NOT NULL, -- 'stdin', 'stdout', 'stderr'
    log_content TEXT NOT NULL,
    exit_code INTEGER,
    FOREIGN KEY (task_request_id) REFERENCES task_requests(id) ON DELETE CASCADE
);
CREATE INDEX IF NOT EXISTS idx_container_operation_logs_task_id ON container_operation_logs (task_request_id);
```

**B. Hono API & D1 Module**

1.  **Bind D1 in `wrangler.jsonc`**:
    ```jsonc
    "d1_databases": [
      {
        "binding": "DB",
        "database_name": "repo-agent-db",
        "database_id": "YOUR_DB_ID_HERE"
      }
    ]
    ```
2.  **Create `src/core/d1.ts`**:
      * Create a module to encapsulate all D1 database operations.
      * Include functions like:
          * `createTaskRequest(db, id, pathway, prompt, repo_url)`
          * `updateTaskStatus(db, id, status)`
          * `logAiOperation(db, task_id, agent_name, ...)`
          * `logContainerOperation(db, task_id, stream, content, ...)`
          * `getTaskStatus(db, id)`
          * `getTaskLogs(db, id)` (joins all log tables)
          * `getRepeatableTasks(db)`
3.  **Refactor Hono API (`src/api/routes.ts`)**:
      * `GET /api/v1/tasks/pathways`: Fetches all from `repeatable_tasks`. (For the frontend).
      * `POST /api/v1/tasks/invoke`:
          * Accepts the JSON payload (prompt, repo\_url, etc.).
          * Generates a new `task_id` (e.g., CUID).
          * Calls `createTaskRequest` to log it in D1.
          * **Crucially**: Does *not* run the task. Instead, it sends the `task_id` to a Queue.
          * Returns a `202 Accepted` with the `task_id` and a link to the WebSocket.
      * `GET /api/v1/tasks/:id/status`: Fetches status from `task_requests`.
      * `GET /api/v1/tasks/:id/logs`: Fetches all logs for a task.

-----

### 2\. Phase 2: Advanced Storage (KV & R2)

Bind KV and R2 in `wrangler.jsonc` to manage state and assets.

```jsonc
"kv_namespaces": [
  {
    "binding": "TASK_CACHE",
    "id": "YOUR_KV_ID_HERE"
  }
],
"r2_buckets": [
  {
    "binding": "CONTAINER_ASSETS",
    "bucket_name": "repo-agent-assets"
  }
]
```

  * **`TASK_CACHE` (KV):** Use this for fast, ephemeral state.
      * **WebSocket Topic:** When a new log is written to D1, also `put` it to KV (e.g., `task_logs:${task_id}`). The WebSocket handler will `get` this key to stream real-time data to the client, which is faster than polling D1.
      * **Session Caching:** Store intermediate results of a repo analysis.
  * **`CONTAINER_ASSETS` (R2):** Use this to store files the container needs to run.
      * Store pre-written bash scripts (e.g., `test_harness.sh`, `validate_patch.sh`).
      * Store Python analysis scripts.
      * The agent's job becomes:
        1.  Generate a presigned URL for the script in R2.
        2.  Tell the sandbox: `curl -L 'PRESIGNED_URL' -o script.sh && bash script.sh`.

-----

### 3\. Phase 3: Agentic Orchestration (Queues, DOs, & Agents)

This is the core logic. You will refactor the API to be asynchronous and managed by a fleet of specialized agents.

**A. Bind Queues and DOs in `wrangler.jsonc`**

```jsonc
"queues": [
  {
    "binding": "TASK_QUEUE",
    "queue_name": "repo-agent-task-queue"
  }
],
"durable_objects": {
  "bindings": [
    // ... your existing "Sandbox" binding
    { "class_name": "Sandbox", "name": "Sandbox" },
    
    // 1. The Main Orchestrator Actor
    { "class_name": "TaskOrchestratorActor", "name": "TASK_ORCHESTRATOR" },
    
    // 2. Specialized Agent Actors
    { "class_name": "ErrorRecreationAgent", "name": "AGENT_ERROR_RECREATION" },
    { "class_name": "SolutionValidationAgent", "name": "AGENT_SOLUTION_VALIDATION" },
    { "class_name": "TestingAgent", "name": "AGENT_TESTING" }
  ]
}
```

**B. Define the Orchestration Flow**

1.  **Hono API (`POST /api/v1/tasks/invoke`)**:

      * Receives request.
      * Creates D1 `task_requests` record.
      * Sends `task_id` to `TASK_QUEUE`.
      * Returns `202 Accepted`.

2.  **Queue Consumer (`src/index.ts`)**:

      * The worker's `queue()` handler receives the message.
      * It gets the `TaskOrchestratorActor` DO stub: `env.TASK_ORCHESTRATOR.get(env.TASK_ORCHESTRATOR.idFromName("main-orchestrator"))`.
      * It calls the actor: `actor.fetch("/start", { method: "POST", body: JSON.stringify({ task_id }) })`.

3.  **`TaskOrchestratorActor` (Durable Object)**:

      * This DO manages the high-level state.
      * Its `fetch` handler receives the `/start` request.
      * It reads the `task_id` from the body.
      * It fetches the full task details from D1.
      * **This is the key step:** It uses the **Cloudflare Agents SDK** (`@cloudflare/agents`) to invoke the *correct* specialized agent.

    <!-- end list -->

    ```typescript
    // Inside TaskOrchestratorActor.ts
    import { Ai } from '@cloudflare/workers-ai';
    import { Agent, CfAgent } from '@cloudflare/agents';

    // ... inside the fetch handler ...
    const task = await getTaskDetails(this.env.DB, task_id);

    let agentBinding;
    switch(task.pathway) {
      case 'error_recreation':
        agentBinding = this.env.AGENT_ERROR_RECREATION;
        break;
      case 'solution_validation':
        agentBinding = this.env.AGENT_SOLUTION_VALIDATION;
        break;
      case 'testing':
        agentBinding = this.env.AGENT_TESTING;
        break;
    }

    // Get the agent (which is also a DO)
    const agent = agentBinding.get(agentBinding.idFromName(task_id));

    // Run the agent
    await agent.fetch("/run", { method: "POST", body: JSON.stringify(task) });
    ```

4.  **Specialized Agents (e.g., `SolutionValidationAgent` DO)**:

      * These are DOs that implement the *actual* logic for a pathway.
      * The `fetch` handler for `/run` is the entry point.
      * **This is where it all comes together**:
        1.  It logs its "thoughts" to D1: `logAiOperation(db, task_id, "SolutionValidationAgent", ... "Cloning repo...")`.
        2.  It gets the `Sandbox` DO: `const sandbox = this.env.Sandbox.get(this.env.Sandbox.idFromName(task_id))`.
        3.  It runs a command: `const cloneResult = await sandbox.exec("git clone ...")`.
        4.  It logs the container output: `logContainerOperation(db, task_id, 'stdout', cloneResult.stdout)`.
        5.  It runs more AI: `const analysis = await ai.run(...)`.
        6.  It logs AI thoughts: `logAiOperation(db, task_id, ... analysis)`.
        7.  It runs more commands (e.g., `apply patch`, `npm test`).
        8.  When finished, it updates the main task status: `updateTaskStatus(db, task_id, 'success')`.

This architecture gives you full traceability (D1), asynchronicity (Queues), state management (DOs), and specialized logic (Agents) while using the Sandbox for secure execution.
